# GCP Logging ETL Pipeline

## ğŸ“Œ Overview
This project is a **real-time ETL pipeline** built on **Google Cloud Platform** to monitor and analyze audit logs from various GCP services.  
It extracts logs from **Cloud Logging (Audit)**, transforms them via a **Cloud Function** (Python), and loads the processed data into **BigQuery** for storage and analysis.

## ğŸ› ï¸ Architecture
Cloud Logging (Audit logs) <br>
â†“ <br>
Logging Sink (Filter)<br>
â†“<br>
Pub/Sub Topic <br>
â†“<br>
Cloud Function (Python - parse & transform)<br>
â†“<br>
BigQuery Table


## ğŸ”§ Services Used
- **Cloud Logging Sink** â€“ Filters logs and routes them to Pub/Sub
- **Pub/Sub** â€“ Decouples logging and processing
- **Cloud Functions (Python)** â€“ Parses logs, extracts metadata, and formats data
- **BigQuery** â€“ Stores structured logs for analytics

## ğŸ“‚ BigQuery Table Schema
| Column       | Type     | Description |
|--------------|----------|-------------|
| resource_name | STRING   | Name of GCP service |
| resource_type | STRING   | Type of service eg. compute |
| operation_type| STRING   | Type of operation (CREATE or DELETE) |
| resource_project| STRING | Project in which the resource was created or deleted. |
| resource_zone | STRING   | Zone of service |


## ğŸš€ Deployment Steps

   ```bash
   gcloud pubsub topics create error-logs
